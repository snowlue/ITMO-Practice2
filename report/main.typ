#let title = "Исследование и разработка методов локализации\nи картирования (SLAM) для мобильной платформы\nна базе LEGO Mindstorms и ROS 2"
#let UDK = "xxx.xxx"
#let year = datetime.today().year()
#let city = "Санкт-Петербург"
#let logoPath = "logo_basic_russian_black.svg"

// Автор
// ИЗМЕНИТЬ!
#let authorFullName1 = "Овчинников Павел Алексеевич"
#let courseNumber1 = "3-го"
#let isuNumber1 = 368606
#let authorFullName2 = "Сайфуллин Динислам Расилевич"
#let courseNumber2 = "2-го"
#let isuNumber2 = 409502

// Руководитель практики
#let supervisorFullName = "Ведяков Алексей Алексеевич"
#let supervisorRegalia = "кандидат технических наук, доцент"

// ВУЗ
#let organization = "Национальный Исследовательский Университет ИТМО"
#let organizationShort = "Университет ИТМО"

// Настройки страницы и др.
#let fontType = "Merriweather"
#let fontSize = 12pt
#let firstLineIndent = 2.5em



// Установка свойств к PDF файлу 
#set document(author: authorFullName1 + " / " + authorFullName2, title: title)

// Форматирование странциы/отступов и др
#set page(
  paper: "a4",
  // размер полей (ГОСТ 7.0.11-2011, 5.3.7)
  margin: (top:2cm, bottom:2cm, left:2cm, right:2cm),
  // Установка сквозной нумерации страниц
  numbering: "1",
)

// Форматирование текста
#set text(
  font: fontType,
  fallback: false, // Включить резеврвные шрифты
  size: fontSize,
  lang: "ru",
  hyphenate: true, // Перенос по словам
)

#set par(
  // Полуторный интервал (ГОСТ 7.0.11-2011, 5.3.6)
  leading: 1em,
  justify: true,
  linebreaks: "optimized",
  // Абзацный отступ. Должен быть одинаковым по всему тексту и равен пяти знакам (ГОСТ Р 7.0.11-2011, 5.3.7).
  first-line-indent: firstLineIndent,
)

#show raw.where(block: true): block.with(
  fill: luma(240),
  inset: 10pt,
  radius: 4pt,
  width: 100%
)
#show raw.where(block: false): box.with(
  fill: luma(240),
  inset: (x: 3pt, y: 0pt),
  outset: (y: 3pt),
  radius: 2pt,
)
#show raw.where(block: true): it => text(9.5pt, it)
#show raw: it => text(fontSize, it)

#show math.equation: set text(fontSize + 2pt)

// Далее добавляются Главы диссертации
#import "Templates/TitlePage.typ": title-page
#title-page(
  organization,
  organizationShort,
  logoPath,
  authorFullName1, courseNumber1, isuNumber1,
  authorFullName2, courseNumber2, isuNumber2,
  title,
  supervisorFullName,
  supervisorRegalia,
  city, year
)

#set par(first-line-indent: 0em)
#show link: underline
#show link: it => text(rgb("#000088"), it)

#outline()
#pagebreak()

#set heading(numbering: none)
= Введение
В этой научно-исследовательской работе рассматривается реализация методов локализации и картирования (SLAM) для мобильной платформы на базе LEGO Mindstorms с использованием ROS 2. Целью работы является создание системы самонавигации робота, способной строить карту окружающей среды и определять свое положение в реальном времени.

*Цель исследования*: исследовать классические алгоритмы ICP, AMCL, и визуальные методы VO SLAM + ArUco для применения на платформе LEGO Mindstorms, разработать модульную архитектуру ROS 2, обеспечивающую интеграцию одометрии, лидара и камеры Realsense D435.

*Задачи исследования*:

+ Сборка и настройка мобильной платформы LEGO Mindstorms, реализация базовой одометрии.
+ Сборка датасета с помощью `rosbag`.
+ Реализация и отладка алгоритмов ICP и AMCL.
+ Настройка визуального SLAM на основе ArUco-маркеров, калибровка камеры Realsense.

#set heading(numbering: "1.")

#pagebreak()
= Теория
В рамках научно-исследовательской работы необходимо разобраться в работе трёх алгоритмов: _ICP_, _AMCL_ и _VO SLAM_. Разберём, как работает каждый из них с теоретической точки зрения.

== ICP
Алгоритм, использующий результаты сканирования лидара в качестве альтернативы одометрии, т.е. для того, чтобы определить как робот повернулся и сдвинулся.

*Входные данные:* два соседних измерения лидара — облака точек. \
*Выходные данные:* сдвиг $t$ и поворот $R$, совмещающие два измерения.

Сам алгоритм на языке псевдокода представлен ниже.

```
Algorithm ICP_Iteration(S_n, S_{n-1}):
    mean_p = mean(S_n)
    mean_q = mean(S_{n-1})
    Cov = 0

    for each point p_i in S_n:
        q_i = argmin over q in S_{n-1} of ||p_i - q||_2
        Cov += (q_i - mean_q)^T * (p_i - mean_p)
    endfor

    [U, Σ, Vh] = SVD(Cov)
    R = Vh^T * U^T
    t = mean_q - R * mean_p

    return t, R
```
#align(center)[Листинг 1: одна итерация алгоритма ICP]

== AMCL
Алгоритм, использующий одометрию робота, результаты сканирования лидара и уже построенную карту вида `binary occupancy grid` для локализации робота.

*Входные данные:* набор частиц на предыдущем шаге, одометрия, измерения лидара, карта окружения \
*Выходные данные:* обновлённый набор частиц на текущем шаге.
#pagebreak()
```
Algorithm Augmented_MCL(χ_{t-1}, u_t, z_t, m):
    static w_slow, w_fast
    χ_bar_t = χ_t = ∅
    for m = 1 to M:
        x_t^[m] = sample_motion_model(u_t, x_{t-1}^[m])
        w_t^[m] = measurement_model(z_t, x_t^[m], m)
        χ_bar_t = χ_bar_t + <x_t^[m], w_t^[m]>
        w_avg = w_avg + (1/M) * w_t^[m]
    endfor
    w_slow = w_slow + α_slow * (w_avg - w_slow)
    w_fast = w_fast + α_fast * (w_avg - w_fast)
    for m = 1 to M:
        with probability max(0.0, 1.0 - w_fast/w_slow):
            add random pose to χ_t
        else:
            draw i ∈ {1,...,N} with probability ∝ w_t^[i]
            add x_t^[i] to χ_t
        endwith
    endfor
    return χ_t
```
#align(center)[Листинг 2: реализация AMCL]

Перед нами адаптивный вариант MCL, который добавляет случайные точки, являющиеся предположением о положении робота. Количество случайных точек определяется путем сравнения краткосрочной и долгосрочной вероятности результатов измерений сенсора.

Как мы видим, для работы алгоритма необходимы `sample_motion_model` и `measurement_model`. Дадим пояснение, за что отвечает каждая из моделей:
- `motion_model` или модель движения — генерирует случайные частицы на известных пустых клетках (помеченные нулями на карте) и использует одометрию робота для того, чтобы соотнести движение частиц с движением робота;
- `measurement_model` или модель измерений — распределяет веса частицам, оценивает их стоимость и занимается их аннигиляцией, основываясь на данных сканирования лидара.

В конечном итоге остаётся некий наиболее весомый набор частиц, близкий к истинному положению робота. Таким образом при движении робота можно локализовать его положение на уже заранее известной карте, которая была получена, в нашем случае, с помощью _ICP_.

Модель движения описывается так:
```
Algorithm sample_motion_model_velocity(u_t, x_{t-1}):
    v_hat = v + sample(α₁|v| + α₂|ω|)
    ω_hat = ω + sample(α₃|v| + α₄|ω|)
    γ_hat = sample(α₅|v| + α₆|ω|)
    
    x' = x - (v_hat / ω_hat) * sin(θ) + (v_hat / ω_hat) * sin(θ + ω_hat * Δt)
    y' = y + (v_hat / ω_hat) * cos(θ) - (v_hat / ω_hat) * cos(θ + ω_hat * Δt)
    θ' = θ + ω_hat * Δt + γ_hat * Δt
    
    return x_i = (x', y', θ')ᵀ
```
#align(center)[Листинг 3: реализация модели движения]
Фактически, это алгоритм сэмплинга позиции $x_t = \(x',y',theta')^T$ из предыдущей $x_(t-1) = (x, y, theta)^T$ и контрольной $u_t=(v, omega)^T$. Конечная ориентация изменяется с помощью дополнительного случайного члена $hat(gamma)$. Переменные от $alpha_1$ до $alpha_6$ являются параметрами шума движения. Функция `sample(b)` генерирует случайную выборку из распределения с нулевым центром с дисперсией $b$. Мы используем выборку с нормальным распределением, которая представлена в блоке кода ниже.
```
Algorithm sample_normal_distribution(b):
    return (b / 6) * sum_{i=1 to 12} rand(-1, 1)
```

В качестве модели измерения, которая будет оценивать качество измерения и улучшать алгоритм, будем использовать модель известного соответствия, которая будет использовать ориентиры (landmarks) и уже известную карту и таким образом уточнять положение робота:
```
Algorithm sample_landmark_model_known_correspondence(f_t^i, c_t^i, m):
    j = c_t^i
    γ_hat = rand(0, 2π)
    r_hat = r_t^i + sample(σ_r²)
    φ_hat = φ_t^i + sample(σ_φ²)
    x = m_{j,x} + r_hat * cos(γ_hat)
    y = m_{j,y} + r_hat * sin(γ_hat)
    θ = γ_hat - π - φ_hat
    return (x, y, θ)ᵀ
```
#align(center)[Листинг 4: реализация модели измерений]

== VO SLAM
Алгоритм получает скорректированное изображение с камеры, ее коэффициенты искажения и матрицу K на вход. С помощью пакета `image_proc` выравниваем изображение и отправляем в алгоритм.

*Входные данные:* скорректированное изображение, коэффициенты искажения, матрицу K, карта окружения \
*Выходные данные:* трансформации ArUco-маркеров.
```
Algorithm Visual_SLAM(frame, K, distCoeffs, m₀):
    gray = to_grayscale(frame)

    corners_list, ids = detectMarkers(gray, marker_size, camera_matrix, dist_coeffs)
    rvecs, tvecs = estimatePose(corners_list, marker_size, camera_matrix, dist_coeffs)
    detected = {}
    for i = 0 to len(ids)-1:
        id = ids[i]
        R = Rodrigues(rvec[i])
        T_cam_m = eye(4)
        T_cam_m[0:3, 0:3] = R
        T_cam_m[0:3, 3] = tvec[i]  
        detected[id] = T_cam_m
    endfor
    
    updateMap(detected)
```
#align(center)[Листинг 5: реализация алгоритма визуального SLAM]
Для ускорения и упрощения детектирования маркеров исходное цветное изображение конвертируется в градации серого. На полученном «сером» кадре вызывается `detectMarkers`, которая возвращает списки углов и соответствующих им уникальных ID маркеров. По найденным углам и известному физическому размеру маркеров с помощью `estimatePose` вычисляются векторы вращения `rvecs` и сдвига `tvecs` относительно камеры. Для каждого маркера собирается однородная матрица `T_cam_m`, которая однозначно задаёт положение и ориентацию маркера в системе координат камеры. Собранный словарь `detected` (где ключ — ID маркера, значение — его матрица трансформации) передаётся функции `updateMap`, которая интегрирует новые или скорректированные положения маркеров в глобальную карту.

```
Algorithm updateMap(detected):
    if map is None:
        first_id = any key from detected
        map[first_id] = detected[first_id]
    else:
        known_ids = intersection(keys(detected), keys(map))
        if known_ids is not None:
            ref = any element of known_ids
            T_cam_ref = detected[ref]
            T_world_ref = map[ref]
            for (id, T_cam_m) in detected.items():
                if id not in map:
                    T_ref_m = inverse(T_cam_ref) @ T_cam_m
                    map[id] = T_world_ref @ T_ref_m
                end if
            end for
        end if
    end if
```
#align(center)[Листинг 6: функция обновления карты с маркерами]
Результатом работы будет карта с положением ArUco-маркеров в пространстве.

#pagebreak()
= Практика
== День 1 (16.06)
В течение первого дня необходимо настроить окружение и изучить существующие решения и литературу касательно ICP и AMCL. Также необходимо собрать данные, которые в дальнейшем будут использоваться для повторного воспроизведения сообщений из топиков.

На ноутбук была поставлена #link("https://neon.kde.org/download")[KDE Neon 6.3], основанная на Ubuntu 24.04 LTS, в которой установлен #link("https://docs.ros.org/en/jazzy/Installation/Ubuntu-Install-Debs.html")[ROS 2 Jazzy]. Также поставлены пакеты и ноды ROS, необходимые для дальнейшей работы:
- самописный `lego_driver` для сокет-взаимодействия с Mindstorms EV3;
- #link("https://wiki.ros.org/teleop_twist_keyboard")[`teleop_twist_keyboard`] для ручного управления роботом;
- #link("https://github.com/Hokuyo-aut/urg_node2")[`urg_node2`] для работы с лидаром Hokuyo;
- #link("https://index.ros.org/p/tf_transformations")[`tf_transformations`] для пространственных преобразований в одометрии
Кроме того был инициализирован #link("https://github.com/snowlue/ITMO-Practice2")[репозиторий в GitHub] и установлен #link("https://obsidian.md")[Obsidian] для ведения логов практики.

Собрали робота на платформе Lego Mindstorms, сверху робота установили лидар (@robot):

#figure(
  grid(
    columns: (1fr, 1fr),
    align(center)[
      #image("attachments/robot1.jpg", height: 30%)
    ],
    align(center)[
      #image("attachments/robot2.jpg", height: 30%)
    ]
  ),
  caption: [Собранный робот]
) <robot>


Датасет для повторного воспроизведения топиков был собран при помощи `rosbag`. Все собранные "бэги" весят много, поэтому для удобства они были выгружены в #link("https://drive.google.com/drive/folders/1JzvNJl-asQq1r2_TtHRjlLN-GFYub50t")[отдельную папку] в Google Диск.

\
== День 2 (17.06)
Разработали `ICPNode`, в рамках которой происходит вычисление ICP для облака точек, которые были записаны через `rosbag` в течение первого дня.

Алгоритм возвращает требуемые нам смещения $d x$, $d y$ и поворот $y a w$ одного облака к другому.
Алгоритм работал неуверенно, поэтому, чтобы отточить его работу, мы вынесли работу ICP в отдельный Python-модуль и там реализовали сохранение gif-анимации и анимацию итераций в `matplotlib`. Результат представлен ниже.

#figure(
  image("attachments/icp_animation.gif", width: 80%),
  caption: [
    Это GIF-анимация --- посмотреть можно #link("https://github.com/snowlue/ITMO-Practice2/blob/main/Obsidian/attachments/icp_animation.gif")[здесь].
  ],
)

Алгоритм хорошо сходился на некотором наборе точек `dst` — для симулирования случайного преобразования и получения `src` был применён сдвиг по $d x = 1$, $d y = 2$ и поворот на $30degree$ против часовой стрелки.

\
== День 3 (18.06)
Как выяснилось, алгоритм хорошо работал в отдельной визуализации, но всё ещё некорректно отрабатывал на данных с лидара в `rviz2`. Это было связано с размерностями и неверным пониманием, что есть `src`, а что `dst`.
- `dst` — облако точек с предыдущего сканирования лидаром;
- `src` — новое облако точек, которое необходимо подвинуть к `dst`.
В случае с алгоритмом, описанным в ICP, `src` является $S_n$, а `dst` является $S_(n-1)$. Таким образом, в течение дня отлаживался алгоритм, чтобы он верно работал:
- Перебор всех ближайших точек был заменён на оптимальный `KDTree` из `sklearn.neighbors`;
- Проверили, что `SVD` в `numpy` работает так, как мы ожидаем, и возвращает правильные матрицы.

Начали записи сканов с лидара в `pkl`-файлы, чтобы воспроизводить сканы в `matplotlib` и отлаживать алгоритм там.

== День 4 (19.06)
Параллельно с ICP приступаем к реализации алгоритма MCL. Создадим визуализацию, чтобы в `matplotlib` можно было увидеть ход работы. Первые неудачные результаты можно увидеть ниже.

#figure(
  image("attachments/bad_amcl.png", width: 60%),
  caption: [
    Результат работы первой версии алгоритма AMCL.
  ],
)

Очевидно, алгоритм требует доработки. Основная проблема заключалась в выставлении ориентиров и скачках ошибки локализации при повороте робота на месте.

Принято решение согласовывать алгоритм ICP совместно с преподавателем. Подбирались различные методы вычисления на определённых шагах — в частности методы вычисления ближайших соседей и фильтрации связей один ко многим, чтобы достичь связей один к одному. Это было необходимо для улучшения работы алгоритма.

\
== День 5 (20.06)
Пятый день ушёл на настройку камеры Intel RealSense D435 Depth Camera и виртуального окружения для него, в частности установка Python 3.11. Был установлен #link("https://pypi.org/project/pyrealsense2/")[`pyrealsense2`] и была проверена работа камеры с помощью #link("https://github.com/lovelyyoshino/SmartCar/blob/master/rs.py")[кода из GitHub].

Начали процесс интеграции в ROS2 и столкнулись с проблемой — ROS пытается использовать установленную системно версию Python 3.12. Использование версий Python и виртуальных окружений через #link("https://docs.astral.sh/uv/")[`uv`], а также попытки запустить ROS2 на версии 3.11 не увенчались успехом.

Было решено использовать встроенную в ROS реализацию realsense — `realsense2_camera`. Сначала устанавливаем необходимые зависимости для работы:
```bash
sudo apt-get install ros-jazzy-realsense2-camera ros-jazzy-realsense2-description
```
И запускаем ноду, работающую с камерой:
```bash
ros2 launch realsense2_camera rs_launch.py pointcloud.enable:=true
```
Затем запускаем узел с включённой генерацией облака точек. Калибровку камеры проверяли с помощью примеров из репозитория: обнаружили, что без привязки к TF-фреймам маркерные трансформации "гуляли" — поэтому необходимо настроить статический трансформ из /camera_link в /base_link, чтобы последующие подсчёты позиций маркеров в глобальной системе шли корректно.

\
== День 6 (23.06)
Шестой день ушёл на доработку ноду с алгоритмом ICP. Основной задачей было конвертировать облако точек `PointCloud` от лидара в `OccupancyGrid` для публикации карты. Сгенерировали сетку 20 × 20 м с разрешением 5 см/ячейка: неизвестные ячейки получили −1, свободные – 0, препятствия – 100.

При реализации столкнулись с проблемами — при каждом новом скане карта составлялась целиком, и свободные области постепенно исчезали. Решением стал переход на накопительную стратегию: храним массив предыдущих значений и обновлем только те ячейки, где действительно появляются новые препятствия или освобождаются участки. Для заполнения свободного пространства была реализована трассировка лучей алгоритмом Брезенхема, а не просто пороговое преобразование глубины: это убрало «пробелы» и дрожание светлых зон при визуализации в RViz.

#figure(
  image("attachments/bad_map.png", width: 100%),
  caption: [
    Результат работы первой версии алгоритма ICP.
  ],
)

Далее были подобраны оптимальные параметры алгоритма для построения карты и отлажен код.
Итоговый результат работы построения карты с помощью лидара:

#figure(
  align(center)[
    #image("attachments/map1.png", width: 100%)
    #image("attachments/map2.png", width: 100%)
  ],
  caption: [
    Результат построения карты.
  ],
)

Под конец дня обсудили с преподавателем работу AMCL с визуализацией в `matplotlib`. Для дальнейшей работы из результата работы ICP были выгружены необходимые данные: карта из топика `/map`, сканы с лидара `/scan` и одометрия `/odom` с корректировкой алгоритма ICP. Карта будет использоваться в `measurement_model`, а одометрия — в `motion_model`.

\
== День 7 (24.06)
Приступаем к разработке ноды `slam_node` для реализации алгоритма VO SLAM. Необходимо считать изображение из топика `/camera/camera/color/image_rect_color` и матрицы преобразования камеры realsense из `/camera/camera/color/camera_info`, чтобы скорректировать искажение камеры, и с помощью `opencv` определять Aruco-маркеры и находить направления их осей. Результат поиска маркеров в `RViz`.  

#figure(
  image("attachments/aruco_detect.png", width: 100%),
  caption: [
    Визуализация отслеживания Aruco-маркеров в `RViz`.
  ],
)

Также в этот день оттачивалась работа алгоритма AMCL и изучалось, как встроить данные с лидара и одометрию робота в `measurement_model` и `motion_model` соответственно. Приходим к выводу, что существующий алгоритм недостаточно хорош для этого, поэтому появилась необходимость в переписании и использовании нового решения.

Руководитель практики отправил #link("https://gitlab.u-angers.fr/cours/mobile_robotic_student/-/blob/master/documents/MCL/MCL.pdf")[новый документ], содержащий описание работы ресемплинга и взаимодействия с картой, одометрией и сканами с лидара. В том же репозитории с документом имеется #link("https://gitlab.u-angers.fr/cours/mobile_robotic_student/-/tree/master/tp_mcl")[шаблон MCL], который был использован для своей реализации.

#pagebreak()
== День 8 (25.06)
Реализован AMCL на реальных данных — с одометрией, сканами с лидара и картой, сформированной через ICP по тем же сканам с лидарам.

#figure(
  image("attachments/Снимок экрана_20250625_173104.png", width: 100%),
  caption: [
    Визуализация работы AMCL на реальных данных и ошибок вычислений.
  ],
) <amcl>
Алгоритм работает в целом неплохо до предпоследнего поворота — исправить эту ошибку, увы, так и не удалось. На @amcl, что после 280-й позиции робота ошибка локализации растёт, а в эволюции весов наблюдаются скачки.

Также в этот день начинаем тестировать алгоритм визуального SLAM, который использует ArUco-маркеры в качестве ориентиров для построения карты окружающего пространства и локализации камеры в этой карте. Для этого вычисляем положение маркеров в системе координат камеры и переводим их в глобальную координатную систему. В процессе обнаружились две главные трудности:
- смещение карты из-за накопления погрешностей между сменами маркеров;
- локализация камеры работает нестабильно.

#figure(
  image("attachments/camera_localization1.png", width: 100%),
  caption: [
    Расположение ArUco-маркеров в глобальных координатах.
  ],
)

Чтобы частично сгладить эти эффекты, стали хранить позы последних маркеров относительно первого распознанного маркера.

\
== День 9 (26.06)
Отладили алгоритм: добавили возможность локализоваться камере относительно ArUco-маркеров на полигоне — положение определяется корректно. Далее был подготовлен полигон для съемки - расклеили по периметру комнаты уникальные ArUco-маркеры примерно на одной высоте. В процессе съемок столкнулись с проблемой, что угол обзора камеры слишком мал из-за чего в кадр попадают только 2 маркера и карта строится неверно:

#figure(
  image("attachments/bad_cam_map.png", width: 90%),
  caption: [
    Первая попытка построения карты алгоритмом VO SLAM.
  ],
)

Пришли к выводу, что необходимо наклеить больше маркеров. Примерное расстояние между ними стало 20-30 см и комната выглядит так:    

#figure(
  image("attachments/room.jpg", height: 42.61%),
  caption: [
    Комната с ArUco-маркерами.
  ],
)

Запустили алгоритм и разными способами снимали карту. Карта строилась "по спирали" и камера неправильно локализовалась на стыке двух маркеров. Самый оптимальный вариант съёмки: вращение камеры на $360degree$ в центре комнаты --- тогда все маркеры оказываются в одной плоскости.

Результат работы алгоритма:

#grid(
  columns: (1fr, 2fr),
  align(center)[
    #image("attachments/room_loc2.jpg", height: 30%)
  ],
  align(center)[
    #image("attachments/camera_localization2.png", width: 100%)
  ]
)

#figure(
  grid(
    columns: (1fr, 2fr),
    align(center)[
      #image("attachments/room_loc3.jpg", height: 30%)
    ],
    align(center)[
      #image("attachments/camera_localization3.png", width: 100%)
    ]
  ),
  caption: [Результат локализации робота]
)

В результате получилось собрать координаты и направление каждого маркера. Камера отлично локализуется в комнате. Это работоспособный каркас для построения карты окружения и локализации, однако требуется дальнейшая доработка фильтрации соответствий, интеграция IMU для компенсации дрейфа и более гибкая стратегия ресемплинга частиц в AMCL при слабой плотности препятствий.


// Выключаем нумерацию списков
#set heading(numbering: none)

\
= Заключение

В результате выполнения работы были реализованы несколько методов SLAM для мобильного робота на базе LEGO Mindstorms и ROS 2.

*Основные достижения:*

- Успешно собраны и настроены аппаратные и программные компоненты платформы: робот, ПО и ноды;
- Разработали пайплайн для работы с роботом:
  1. Соединение с роботом по SSH
  2. Запуск `lego_driver` локально
  3. Запуск `sockClient` на роботе
  4. Загрузка `urg_node2` для работы лидара
  5. Запуск `teleop_twist_keyboard` для управления роботом с ноутбука
  6. Запуск `rviz2` для визуализации
- Собраны датасеты для повторного воспроизведения сообщений из топиков;
- Реализованы алгоритмы ICP и AMCL, разработаны соответствующие ноды;
- Настроен визуальный SLAM с использованием ArUco-маркеров;
- Проведены эксперименты, предложены направления доработки.

Данная работа создаёт прочную основу для дальнейшего развития гибридных SLAM-систем на мобильных платформах. В перспективе возможно внедрение алгоритмов на платформу, использование нейросетевых методов для извлечения признаков и повышение устойчивости в динамических условиях.

#pagebreak()

#bibliography("works.bib", title: "Список используемой литературы", full: true)